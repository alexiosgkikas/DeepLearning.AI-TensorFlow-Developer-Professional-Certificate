{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Course 3 Week 1 </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'love': 1, 'my': 2, 'i': 3, 'dog': 4, 'cat': 5, 'you': 6}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sentences =[\n",
    "    'I love my dog',\n",
    "    'I love my cat',\n",
    "    'You love my dog!',\n",
    "]\n",
    "\n",
    "tokenizer = Tokenizer(num_words=100) # top 100 volume words\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "word_index = tokenizer.word_index\n",
    "print(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'my': 1, 'love': 2, 'dog': 3, 'i': 4, 'you': 5, 'cat': 6, 'do': 7, 'think': 8, 'is': 9, 'amazing': 10}\n",
      "[[4, 2, 1, 3], [4, 2, 1, 6], [5, 2, 1, 3], [7, 5, 8, 1, 3, 9, 10]]\n",
      "[[4, 2, 1, 3], [1, 3, 1]]\n"
     ]
    }
   ],
   "source": [
    "sentences =[\n",
    "    'I love my dog',\n",
    "    'I love my cat',\n",
    "    'You love my dog!',\n",
    "    'Do you think my dog is amazing?',\n",
    "]\n",
    "\n",
    "tokenizer = Tokenizer(num_words=100) # top 100 volume words\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "word_index = tokenizer.word_index\n",
    "#text to sequence\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "\n",
    "print(word_index)\n",
    "print(sequences)\n",
    "\n",
    "test_data =[\n",
    "     'I really love my dog',\n",
    "     'my dog loves my mantee'\n",
    "]\n",
    "test_seq = tokenizer.texts_to_sequences(test_data)\n",
    "# word \"really\" is lost cz there is not in dict\n",
    "print(test_seq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<OO>': 1, 'my': 2, 'love': 3, 'dog': 4, 'i': 5, 'you': 6, 'cat': 7, 'do': 8, 'think': 9, 'is': 10, 'amazing': 11}\n",
      "[[5, 3, 2, 4], [5, 3, 2, 7], [6, 3, 2, 4], [8, 6, 9, 2, 4, 10, 11]]\n",
      "[[5, 1, 3, 2, 4], [2, 4, 1, 2, 1]]\n"
     ]
    }
   ],
   "source": [
    "sentences =[\n",
    "    'I love my dog',\n",
    "    'I love my cat',\n",
    "    'You love my dog!',\n",
    "    'Do you think my dog is amazing?',\n",
    "]\n",
    "# add a special character \"<OOV>\" , so words that doesnt in dict get this value\n",
    "tokenizer = Tokenizer(num_words=100,oov_token=\"<OO>\") # top 100 volume words\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "word_index = tokenizer.word_index\n",
    "#text to sequence\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "\n",
    "print(word_index)\n",
    "print(sequences)\n",
    "\n",
    "test_data =[\n",
    "     'I really love my dog',\n",
    "     'my dog loves my mantee'\n",
    "]\n",
    "test_seq = tokenizer.texts_to_sequences(test_data)\n",
    "# word \"really\" is lost cz there is not in dict\n",
    "print(test_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<OO>': 1, 'my': 2, 'love': 3, 'dog': 4, 'i': 5, 'you': 6, 'cat': 7, 'do': 8, 'think': 9, 'is': 10, 'amazing': 11}\n",
      "[[5, 3, 2, 4], [5, 3, 2, 7], [6, 3, 2, 4], [8, 6, 9, 2, 4, 10, 11]]\n",
      "[[ 0  0  0  5  3  2  4]\n",
      " [ 0  0  0  5  3  2  7]\n",
      " [ 0  0  0  6  3  2  4]\n",
      " [ 8  6  9  2  4 10 11]]\n",
      "[[5 3 2 4 0]\n",
      " [5 3 2 7 0]\n",
      " [6 3 2 4 0]\n",
      " [8 6 9 2 4]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "sentences =[\n",
    "    'I love my dog',\n",
    "    'I love my cat',\n",
    "    'You love my dog!',\n",
    "    'Do you think my dog is amazing?',\n",
    "]\n",
    "# add a special character \"<OOV>\" , so words that doesnt in dict get this value\n",
    "tokenizer = Tokenizer(num_words=100,oov_token=\"<OO>\") # top 100 volume words\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "word_index = tokenizer.word_index\n",
    "#text to sequence\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "#padded , all senctences have the same length\n",
    "padded = pad_sequences(sequences)\n",
    "print(word_index)\n",
    "print(sequences)\n",
    "print(padded)\n",
    "\n",
    "padded = pad_sequences(sequences,padding='post',\n",
    "                       truncating='post',maxlen=5)\n",
    "print(padded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--2020-10-07 17:32:16--  https://storage.googleapis.com/laurencemoroney-blog.appspot.com/sarcasm.json\n",
      "Resolving storage.googleapis.com (storage.googleapis.com)... 2a00:1450:4017:80b::2010, 2a00:1450:4017:808::2010, 2a00:1450:4017:809::2010, ...\n",
      "Connecting to storage.googleapis.com (storage.googleapis.com)|2a00:1450:4017:80b::2010|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 5643545 (5.4M) [application/json]\n",
      "Saving to: '/tmp/sarcasm.json'\n",
      "\n",
      "     0K .......... .......... .......... .......... ..........  0%  774K 7s\n",
      "    50K .......... .......... .......... .......... ..........  1% 1.55M 5s\n",
      "   100K .......... .......... .......... .......... ..........  2% 1.57M 5s\n",
      "   150K .......... .......... .......... .......... ..........  3%  597K 6s\n",
      "   200K .......... .......... .......... .......... ..........  4% 2.63M 5s\n",
      "   250K .......... .......... .......... .......... ..........  5% 3.19M 4s\n",
      "   300K .......... .......... .......... .......... ..........  6%  666K 5s\n",
      "   350K .......... .......... .......... .......... ..........  7%  761K 5s\n",
      "   400K .......... .......... .......... .......... ..........  8% 1.61M 5s\n",
      "   450K .......... .......... .......... .......... ..........  9%  656K 5s\n",
      "   500K .......... .......... .......... .......... ..........  9% 1.60M 5s\n",
      "   550K .......... .......... .......... .......... .......... 10% 1.62M 5s\n",
      "   600K .......... .......... .......... .......... .......... 11% 1.57M 4s\n",
      "   650K .......... .......... .......... .......... .......... 12% 1.60M 4s\n",
      "   700K .......... .......... .......... .......... .......... 13% 1.59M 4s\n",
      "   750K .......... .......... .......... .......... .......... 14% 1.59M 4s\n",
      "   800K .......... .......... .......... .......... .......... 15% 1.59M 4s\n",
      "   850K .......... .......... .......... .......... .......... 16% 1.62M 4s\n",
      "   900K .......... .......... .......... .......... .......... 17% 1.60M 4s\n",
      "   950K .......... .......... .......... .......... .......... 18%  611K 4s\n",
      "  1000K .......... .......... .......... .......... .......... 19% 5.24M 4s\n",
      "  1050K .......... .......... .......... .......... .......... 19% 29.0M 3s\n",
      "  1100K .......... .......... .......... .......... .......... 20%  757K 4s\n",
      "  1150K .......... .......... .......... .......... .......... 21% 1.08M 3s\n",
      "  1200K .......... .......... .......... .......... .......... 22% 1.65M 3s\n",
      "  1250K .......... .......... .......... .......... .......... 23% 1.69M 3s\n",
      "  1300K .......... .......... .......... .......... .......... 24% 1.70M 3s\n",
      "  1350K .......... .......... .......... .......... .......... 25% 1.68M 3s\n",
      "  1400K .......... .......... .......... .......... .......... 26% 1.70M 3s\n",
      "  1450K .......... .......... .......... .......... .......... 27% 1.65M 3s\n",
      "  1500K .......... .......... .......... .......... .......... 28%  576K 3s\n",
      "  1550K .......... .......... .......... .......... .......... 29% 1.49M 3s\n",
      "  1600K .......... .......... .......... .......... .......... 29% 1.19M 3s\n",
      "  1650K .......... .......... .......... .......... .......... 30%  148M 3s\n",
      "  1700K .......... .......... .......... .......... .......... 31% 3.94M 3s\n",
      "  1750K .......... .......... .......... .......... .......... 32% 1.12M 3s\n",
      "  1800K .......... .......... .......... .......... .......... 33% 1.11M 3s\n",
      "  1850K .......... .......... .......... .......... .......... 34% 1.54M 3s\n",
      "  1900K .......... .......... .......... .......... .......... 35% 1.72M 3s\n",
      "  1950K .......... .......... .......... .......... .......... 36% 1.26M 3s\n",
      "  2000K .......... .......... .......... .......... .......... 37% 1.72M 3s\n",
      "  2050K .......... .......... .......... .......... .......... 38% 1.65M 3s\n",
      "  2100K .......... .......... .......... .......... .......... 39% 1.71M 2s\n",
      "  2150K .......... .......... .......... .......... .......... 39% 1.69M 2s\n",
      "  2200K .......... .......... .......... .......... .......... 40% 1.68M 2s\n",
      "  2250K .......... .......... .......... .......... .......... 41% 1.69M 2s\n",
      "  2300K .......... .......... .......... .......... .......... 42% 1.70M 2s\n",
      "  2350K .......... .......... .......... .......... .......... 43%  555K 2s\n",
      "  2400K .......... .......... .......... .......... .......... 44% 2.28M 2s\n",
      "  2450K .......... .......... .......... .......... .......... 45%  212M 2s\n",
      "  2500K .......... .......... .......... .......... .......... 46% 1.52M 2s\n",
      "  2550K .......... .......... .......... .......... .......... 47% 1.21M 2s\n",
      "  2600K .......... .......... .......... .......... .......... 48% 1.44M 2s\n",
      "  2650K .......... .......... .......... .......... .......... 48% 1.72M 2s\n",
      "  2700K .......... .......... .......... .......... .......... 49% 1.48M 2s\n",
      "  2750K .......... .......... .......... .......... .......... 50%  657K 2s\n",
      "  2800K .......... .......... .......... .......... .......... 51%  144M 2s\n",
      "  2850K .......... .......... .......... .......... .......... 52% 1.63M 2s\n",
      "  2900K .......... .......... .......... .......... .......... 53% 1007K 2s\n",
      "  2950K .......... .......... .......... .......... .......... 54% 1.28M 2s\n",
      "  3000K .......... .......... .......... .......... .......... 55% 1.73M 2s\n",
      "  3050K .......... .......... .......... .......... .......... 56% 1.36M 2s\n",
      "  3100K .......... .......... .......... .......... .......... 57% 1.63M 2s\n",
      "  3150K .......... .......... .......... .......... .......... 58% 1.29M 2s\n",
      "  3200K .......... .......... .......... .......... .......... 58% 1.66M 2s\n",
      "  3250K .......... .......... .......... .......... .......... 59% 1.70M 2s\n",
      "  3300K .......... .......... .......... .......... .......... 60% 1.71M 2s\n",
      "  3350K .......... .......... .......... .......... .......... 61% 1.68M 2s\n",
      "  3400K .......... .......... .......... .......... .......... 62%  599K 2s\n",
      "  3450K .......... .......... .......... .......... .......... 63% 2.39M 1s\n",
      "  3500K .......... .......... .......... .......... .......... 64% 13.7M 1s\n",
      "  3550K .......... .......... .......... .......... .......... 65% 1.58M 1s\n",
      "  3600K .......... .......... .......... .......... .......... 66% 1.04M 1s\n",
      "  3650K .......... .......... .......... .......... .......... 67% 1.47M 1s\n",
      "  3700K .......... .......... .......... .......... .......... 68% 1.66M 1s\n",
      "  3750K .......... .......... .......... .......... .......... 68% 1.72M 1s\n",
      "  3800K .......... .......... .......... .......... .......... 69% 1.70M 1s\n",
      "  3850K .......... .......... .......... .......... .......... 70% 1.65M 1s\n",
      "  3900K .......... .......... .......... .......... .......... 71% 1.71M 1s\n",
      "  3950K .......... .......... .......... .......... .......... 72% 1.26M 1s\n",
      "  4000K .......... .......... .......... .......... .......... 73% 1.71M 1s\n",
      "  4050K .......... .......... .......... .......... .......... 74% 1.71M 1s\n",
      "  4100K .......... .......... .......... .......... .......... 75%  635K 1s\n",
      "  4150K .......... .......... .......... .......... .......... 76% 1.81M 1s\n",
      "  4200K .......... .......... .......... .......... .......... 77% 1.60M 1s\n",
      "  4250K .......... .......... .......... .......... .......... 78%  213M 1s\n",
      "  4300K .......... .......... .......... .......... .......... 78% 1.41M 1s\n",
      "  4350K .......... .......... .......... .......... .......... 79% 1002K 1s\n",
      "  4400K .......... .......... .......... .......... .......... 80% 1.61M 1s\n",
      "  4450K .......... .......... .......... .......... .......... 81% 1.67M 1s\n",
      "  4500K .......... .......... .......... .......... .......... 82% 1.71M 1s\n",
      "  4550K .......... .......... .......... .......... .......... 83% 1.71M 1s\n",
      "  4600K .......... .......... .......... .......... .......... 84% 1.66M 1s\n",
      "  4650K .......... .......... .......... .......... .......... 85% 1.73M 1s\n",
      "  4700K .......... .......... .......... .......... .......... 86% 1.69M 1s\n",
      "  4750K .......... .......... .......... .......... .......... 87% 1.26M 0s\n",
      "  4800K .......... .......... .......... .......... .......... 88% 1.66M 0s\n",
      "  4850K .......... .......... .......... .......... .......... 88%  672K 0s\n",
      "  4900K .......... .......... .......... .......... .......... 89% 1.45M 0s\n",
      "  4950K .......... .......... .......... .......... .......... 90% 4.01M 0s\n",
      "  5000K .......... .......... .......... .......... .......... 91% 3.53M 0s\n",
      "  5050K .......... .......... .......... .......... .......... 92% 1.14M 0s\n",
      "  5100K .......... .......... .......... .......... .......... 93% 1.42M 0s\n",
      "  5150K .......... .......... .......... .......... .......... 94% 1.23M 0s\n",
      "  5200K .......... .......... .......... .......... .......... 95% 1.67M 0s\n",
      "  5250K .......... .......... .......... .......... .......... 96% 1.71M 0s\n",
      "  5300K .......... .......... .......... .......... .......... 97% 1.71M 0s\n",
      "  5350K .......... .......... .......... .......... .......... 97% 1.66M 0s\n",
      "  5400K .......... .......... .......... .......... .......... 98% 1.73M 0s\n",
      "  5450K .......... .......... .......... .......... .......... 99% 1.69M 0s\n",
      "  5500K .......... .                                          100% 2.05M=3.8s\n",
      "\n",
      "2020-10-07 17:32:20 (1.41 MB/s) - '/tmp/sarcasm.json' saved [5643545/5643545]\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'r' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-cad5caa8781a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'sarcasm.json'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mdatastore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'r' is not defined"
     ]
    }
   ],
   "source": [
    "#SARCASM\n",
    "!wget --no-check-certificate \\\n",
    "    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/sarcasm.json \\\n",
    "    -O /tmp/sarcasm.json\n",
    "\n",
    "\n",
    "import json\n",
    "\n",
    "with open('sarcasm.json',r) as f:\n",
    "    datastore = json.load(f)\n",
    "\n",
    "setences = []\n",
    "labels = []\n",
    "urls = []\n",
    "for item in datastore:\n",
    "    setences.append(item['headline'])\n",
    "    labels.append(item['is_sarcastic'])\n",
    "    urls.append(item['article_link'])\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(oov_token=\"<OO>\")\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "#padded , all senctences have the same length\n",
    "padded = pad_sequences(sequences,padding='post')\n",
    "print(padded[0])\n",
    "print(padded.shape)\n",
    "import csv\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "#Stopwords list from https://github.com/Yoast/YoastSEO.js/blob/develop/src/config/stopwords.js\n",
    "# Convert it to a Python list and paste it here\n",
    "stopwords = [ \"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"could\", \"did\", \"do\", \"does\", \"doing\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\", \"is\", \"it\", \"it's\", \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \"so\", \"some\", \"such\", \"than\", \"that\", \"that's\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"there's\", \"these\", \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"were\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\", \"which\", \"while\", \"who\", \"who's\", \"whom\", \"why\", \"why's\", \"with\", \"would\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\" ]#YOUR CODE HERE\n",
    "print(type(stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "153\n",
      "2225\n",
      "tv future hands viewers home theatre systems  plasma high-definition tvs  digital video recorders moving living room  way people watch tv will radically different five years  time.  according expert panel gathered annual consumer electronics show las vegas discuss new technologies will impact one favourite pastimes. us leading trend  programmes content will delivered viewers via home networks  cable  satellite  telecoms companies  broadband service providers front rooms portable devices.  one talked-about technologies ces digital personal video recorders (dvr pvr). set-top boxes  like us s tivo uk s sky+ system  allow people record  store  play  pause forward wind tv programmes want.  essentially  technology allows much personalised tv. also built-in high-definition tv sets  big business japan us  slower take off europe lack high-definition programming. not can people forward wind adverts  can also forget abiding network channel schedules  putting together a-la-carte entertainment. us networks cable satellite companies worried means terms advertising revenues well  brand identity  viewer loyalty channels. although us leads technology moment  also concern raised europe  particularly growing uptake services like sky+.  happens today  will see nine months years  time uk   adam hume  bbc broadcast s futurologist told bbc news website. likes bbc  no issues lost advertising revenue yet. pressing issue moment commercial uk broadcasters  brand loyalty important everyone.  will talking content brands rather network brands   said tim hanlon  brand communications firm starcom mediavest.  reality broadband connections  anybody can producer content.  added:  challenge now hard promote programme much choice.   means  said stacey jolna  senior vice president tv guide tv group  way people find content want watch simplified tv viewers. means networks  us terms  channels take leaf google s book search engine future  instead scheduler help people find want watch. kind channel model might work younger ipod generation used taking control gadgets play them. might not suit everyone  panel recognised. older generations comfortable familiar schedules channel brands know getting. perhaps not want much choice put hands  mr hanlon suggested.  end  kids just diapers pushing buttons already - everything possible available   said mr hanlon.  ultimately  consumer will tell market want.   50 000 new gadgets technologies showcased ces  many enhancing tv-watching experience. high-definition tv sets everywhere many new models lcd (liquid crystal display) tvs launched dvr capability built  instead external boxes. one example launched show humax s 26-inch lcd tv 80-hour tivo dvr dvd recorder. one us s biggest satellite tv companies  directtv  even launched branded dvr show 100-hours recording capability  instant replay  search function. set can pause rewind tv 90 hours. microsoft chief bill gates announced pre-show keynote speech partnership tivo  called tivotogo  means people can play recorded programmes windows pcs mobile devices. reflect increasing trend freeing multimedia people can watch want  want.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "#Stopwords list from https://github.com/Yoast/YoastSEO.js/blob/develop/src/config/stopwords.js\n",
    "# Convert it to a Python list and paste it here\n",
    "stopwords = [ \"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"could\", \"did\", \"do\", \"does\", \"doing\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\", \"is\", \"it\", \"it's\", \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \"so\", \"some\", \"such\", \"than\", \"that\", \"that's\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"there's\", \"these\", \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"were\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\", \"which\", \"while\", \"who\", \"who's\", \"whom\", \"why\", \"why's\", \"with\", \"would\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\" ]#YOUR CODE HERE\n",
    "print(type(stopwords))\n",
    "print(len(stopwords))\n",
    "\n",
    "sentences = []\n",
    "labels = []\n",
    "with open(\"bbc-text.csv\", 'r') as csvfile:\n",
    "    # Your Code here\n",
    "    reader = csv.reader(csvfile, delimiter=',')\n",
    "    next(reader)\n",
    "    for row in reader:\n",
    "        labels.append(row[0])\n",
    "        filtered_sentence = row[1]\n",
    "        for stopword in stopwords:\n",
    "            filtered_sentence=filtered_sentence.replace(' '+stopword+' ' , ' ')\n",
    "        sentences.append(filtered_sentence.replace('  ',' '))\n",
    "\n",
    "\n",
    "print(len(sentences))\n",
    "print(sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.1\n"
     ]
    }
   ],
   "source": [
    "!pip install -q tensorflow-datasets\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset imdb_reviews/plain_text/1.0.0 (download: Unknown size, generated: Unknown size, total: Unknown size) to C:\\Users\\Alexios\\tensorflow_datasets\\imdb_reviews\\plain_text\\1.0.0...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca7c07c98af544b1b8402c88cbbf3657",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Dl Completed...', max=1.0, style=Progre…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eced445ebf104520b6a7646a9b5e491b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Dl Size...', max=1.0, style=ProgressSty…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d90be48808e64b4cb5907ff505b367ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffling and writing examples to C:\\Users\\Alexios\\tensorflow_datasets\\imdb_reviews\\plain_text\\1.0.0.incomplete1DUK0E\\imdb_reviews-train.tfrecord\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "885fb6caf16945b8a2f6266245354211",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=25000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84b29ac4d95a4d8cb5fe4c3dc1619cb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffling and writing examples to C:\\Users\\Alexios\\tensorflow_datasets\\imdb_reviews\\plain_text\\1.0.0.incomplete1DUK0E\\imdb_reviews-test.tfrecord\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d551b80fcc647f2b9ec2954e4b84a9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=25000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2952bf34906b4ebba748d28deca26438",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffling and writing examples to C:\\Users\\Alexios\\tensorflow_datasets\\imdb_reviews\\plain_text\\1.0.0.incomplete1DUK0E\\imdb_reviews-unsupervised.tfrecord\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "307a14653f9a4dda959040438c404dd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=50000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Dataset is using deprecated text encoder API which will be removed soon. Please use the plain_text version of the dataset and migrate to `tensorflow_text`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset imdb_reviews downloaded and prepared to C:\\Users\\Alexios\\tensorflow_datasets\\imdb_reviews\\plain_text\\1.0.0. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "imdb,info = tfds.load('imdb_reviews', with_info=True, as_supervised=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = imdb['train'], imdb['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_setences =[]\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
